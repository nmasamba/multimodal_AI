{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e70aaa-ea36-4d3d-af61-3f8d93cf9d0c",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. The input to models supporting this task is typically a combination of an image and a question, and the output is an answer expressed in natural language.\n",
    "\n",
    "Some noteworthy use case examples for VQA include:\n",
    "- Accessibility applications for visually impaired individuals.\n",
    "- Education: posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.\n",
    "- Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products.\n",
    "- Image retrieval: VQA models can be used to retrieve images with specific characteristics. For example, the user can ask “Is there a dog?” to find all images with dogs from a set of images.\n",
    "\n",
    "In this guide:\n",
    "1. Fine-tune a classification VQA model, specifically ViLT, on the Graphcore/vqa dataset.\n",
    "2. Use your fine-tuned ViLT for inference.\n",
    "3. Run zero-shot VQA inference with a generative model, like BLIP-2.\n",
    "\n",
    "A note on ViLT versus some recent VQA models: ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier head is placed on top (a linear layer on top of the final hidden state of the [CLS] token) and randomly initialized. Visual Question Answering is thus treated as a classification problem. More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we illustrate how to use them for zero-shot VQA inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c5309-8831-406d-bcf1-1f6e5ddb3b86",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee66dfd-11c7-4286-8c22-ff22e4849f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21265c-f76e-429f-b151-bedbca9a1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import ViltProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11caf23-d81a-41e4-ac1f-6e4449673c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "MODEL_CHECKPOINT = \"dandelin/vilt-b32-mlm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcee77f-d277-42c8-b822-4950fb9556bb",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00c6ed-62b9-4f8a-b947-fa4105e013e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a very small sample of the annotated visual question answering Graphcore/vqa dataset\n",
    "dataset = load_dataset(\"Graphcore/vqa\", split=\"validation[:200]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1abf8-081a-4827-b057-ccd4fb9a8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect an example\n",
    "# The features relevant to the task include:\n",
    "# question: the question to be answered from the image\n",
    "# image_id: the path to the image the question refers to\n",
    "# label: the annotations (contains several answers to the same question because answers can be subjective)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9faf3-d0f2-4d6a-9d81-c54f6c14e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the image corresponding to the example above. What label would you have given for the question?\n",
    "image = Image.open(dataset[0]['image_id'])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159d0a0-abbc-455d-9677-4e6a6e0741dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rest of the features as they won't be necessary for this task\n",
    "dataset = dataset.remove_columns(['question_type', 'question_id', 'answer_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab70257-999c-4166-bf52-bc9d5b200379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to Q&As' ambiguity, datasets like this are treated as a multi-label classification problem\n",
    "# Moreover, rather than just creating a one-hot encoded vector, one creates a soft encoding\n",
    "# Soft encoding based on the number of times a certain answer appeared in the annotations\n",
    "labels = [item['ids'] for item in dataset['label']]\n",
    "flattened_labels = list(itertools.chain(*labels))\n",
    "unique_labels = list(set(flattened_labels))\n",
    "\n",
    "# To later instantiate the model with an appropriate classification head, create two dictionaries\n",
    "# One dictionary maps the label name to an integer, and the other reverses this mapping\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b0b9a-ecd1-4684-b22e-4ed9a56c99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the mappings, we can replace the string answers with their ids\n",
    "def replace_ids(inputs):\n",
    "    inputs[\"label\"][\"ids\"] = [label2id[x] for x in inputs[\"label\"][\"ids\"]]\n",
    "    return inputs\n",
    "\n",
    "dataset = dataset.map(replace_ids)\n",
    "flat_dataset = dataset.flatten()\n",
    "flat_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97d0ea-4b2a-44a6-9ee3-0021ccb57c48",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0f290-f19c-4381-b3b7-43a209a18f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a ViLT processor to prepare the image and text data\n",
    "# ViltProcessor wraps a BERT tokenizer and ViLT image processor into a convenient single processor\n",
    "processor = ViltProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1f7ff-9429-41cc-95f1-87a2cb5f74ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad76696-88a1-49b1-9576-22e93bb4ffa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3716f-6833-4e99-988a-195efe098861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20763ce4-10c5-475d-addf-566071223c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
