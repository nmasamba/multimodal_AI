{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e70aaa-ea36-4d3d-af61-3f8d93cf9d0c",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. The input to models supporting this task is typically a combination of an image and a question, and the output is an answer expressed in natural language.\n",
    "\n",
    "Some noteworthy use case examples for VQA include:\n",
    "- Accessibility applications for visually impaired individuals.\n",
    "- Education: posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.\n",
    "- Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products.\n",
    "- Image retrieval: VQA models can be used to retrieve images with specific characteristics. For example, the user can ask “Is there a dog?” to find all images with dogs from a set of images.\n",
    "\n",
    "In this guide:\n",
    "1. Fine-tune a classification VQA model, specifically ViLT, on the Graphcore/vqa dataset.\n",
    "2. Use your fine-tuned ViLT for inference.\n",
    "3. Run zero-shot VQA inference with a generative model, like BLIP-2.\n",
    "\n",
    "A note on fine-tuning ViLT:\n",
    "ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier head is placed on top (a linear layer on top of the final hidden state of the [CLS] token) and randomly initialized. Visual Question Answering is thus treated as a classification problem.\n",
    "\n",
    "More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we illustrate how to use them for zero-shot VQA inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c5309-8831-406d-bcf1-1f6e5ddb3b86",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
