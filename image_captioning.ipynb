{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19e8ca4-b3aa-4d2a-8bcb-32537c2c529f",
   "metadata": {},
   "source": [
    "Image captioning is the task of predicting a caption for a given image. Common real world applications of it include aiding visually impaired people that can help them navigate through different situations. Therefore, image captioning helps to improve content accessibility for people by describing images to them.\n",
    "\n",
    "This guide shows how to:\n",
    "1. Fine-tune an image captioning model.\n",
    "2. Use the fine-tuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92750bfc-7835-413b-8e12-c68394189e04",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11d63d-c206-4584-8209-896d18c4d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate -q\n",
    "pip install jiwer -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9642c1-354e-4ed4-8cab-dd77a01b21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from textwrap import wrap\n",
    "from evaluate import load\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cc91b-3bf6-461d-b2cc-a5fbbe270159",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33769a3-f5cf-4b56-8c1e-fdb4252dc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pok√©mon BLIP captions dataset\n",
    "# Consists of {image-caption} pairs\n",
    "ds = load_dataset(\"lambdalabs/pokemon-blip-captions\")\n",
    "\n",
    "# Inspect data set - note the two features (image and text)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504a470-a7af-48a8-a7ff-661f38433fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c658d-5057-4329-8fc8-5392834650a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise examples from the training set\n",
    "def plot_images(images, captions):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, 12))\n",
    "        plt.title(caption)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "sample_images_to_visualize = [np.array(train_ds[i][\"image\"]) for i in range(5)]\n",
    "sample_captions = [train_ds[i][\"text\"] for i in range(5)]\n",
    "plot_images(sample_images_to_visualize, sample_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53fd026-a4c7-450d-a84e-66a25e663dd8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f4a14-ab7d-4dd7-84dd-2c564842fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model pre-processor\n",
    "checkpoint = \"microsoft/git-base\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db7493-92ec-46a0-b135-84a5004dc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set has two modalities (image and text) that need pre-processing\n",
    "# Write a function to pre-process the image (which includes resizing, and pixel scaling) and tokenize the caption\n",
    "def transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    captions = [x for x in example_batch[\"text\"]]\n",
    "    inputs = processor(images=images, text=captions, padding=\"max_length\")\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs\n",
    "\n",
    "train_ds.set_transform(transforms)\n",
    "test_ds.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3d24f-cadf-481d-adbd-c1056bfff58f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c4fbd-812e-4caa-8b50-b99475d1bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image captioning models are typically evaluated with the Rouge Score or Word Error Rate\n",
    "# In this tutorial, we use Word Error Rate for eval\n",
    "\n",
    "wer = load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predicted = logits.argmax(-1)\n",
    "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)\n",
    "    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "    return {\"wer_score\": wer_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ea5ad-02c5-4508-80a7-1249d833721c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ebf8f-5de7-4979-b06a-0005aba5dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ea06f-3bb0-4f26-a381-50686406821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "model_name = checkpoint.split(\"/\")[1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-pokemon\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=50,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Pass training arguments to the trainer along with data set and model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Call trainer to start training/fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf6a6e-4e47-4648-a4c6-87cece4b8ab6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54deb34e-bcba-4b95-971b-537ac498f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image for captioning\n",
    "url = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370eb03-23e0-4db1-a2ad-505f4f6dab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process image and pass to model\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437f6ae-6d72-494b-be90-1df5aef78d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call generate() on the model to predictively caption\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
