{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19531d0d-8841-4923-b895-13069598539b",
   "metadata": {},
   "source": [
    "Document Question Answering, also referred to as Document Visual Question Answering, is a task that involves providing answers to questions posed about document images. The input to models supporting this task is typically a combination of an image and a question, and the output is an answer expressed in natural language. These models utilize multiple modalities, including text, the positions of words (bounding boxes), and the image itself.\n",
    "\n",
    "This guide illustrates how to:\n",
    "- Fine-tune LayoutLMv2 on the DocVQA dataset.\n",
    "- Use the fine-tuned model for inference.\n",
    "\n",
    "LayoutLMv2 solves the document question-answering task by adding a question-answering head on top of the final hidden states of the tokens, to predict the positions of the start and end tokens of the answer. In other words, the problem is treated as extractive question answering: given the context, extract which piece of information answers the question. The context comes from the output of an OCR engine, which is Google’s Tesseract in this particular case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46074775-1a70-456d-b5c9-ca4ae142e0e1",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b8e311-41bc-4cfd-86cc-b6df57642ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326cf8d9-d5f8-46ec-bc41-a985c878433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'detectron2'...\n",
      "remote: Enumerating objects: 15819, done.\u001b[K\n",
      "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
      "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
      "remote: Total 15819 (delta 31), reused 47 (delta 17), pack-reused 15743 (from 1)\u001b[K\n",
      "Receiving objects: 100% (15819/15819), 6.38 MiB | 24.30 MiB/s, done.\n",
      "Resolving deltas: 100% (11525/11525), done.\n",
      "Obtaining file:///Users/nm/Projects/multimodal_AI/multimodal_AI/detectron2\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from detectron2==0.6) (10.0.1)\n",
      "Requirement already satisfied: matplotlib in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from detectron2==0.6) (3.8.0)\n",
      "Collecting pycocotools>=2.0.2 (from detectron2==0.6)\n",
      "  Using cached pycocotools-2.0.8-cp310-cp310-macosx_10_9_universal2.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: termcolor>=1.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from detectron2==0.6) (2.1.0)\n",
      "Requirement already satisfied: yacs>=0.1.8 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from detectron2==0.6) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from detectron2==0.6) (0.9.0)\n",
      "Collecting cloudpickle (from detectron2==0.6)\n",
      "  Using cached cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from detectron2==0.6) (4.65.0)\n",
      "Requirement already satisfied: tensorboard in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from detectron2==0.6) (2.10.0)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
      "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
      "  Using cached iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
      "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting black (from detectron2==0.6)\n",
      "  Using cached black-24.10.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (79 kB)\n",
      "Requirement already satisfied: packaging in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from detectron2==0.6) (23.1)\n",
      "Requirement already satisfied: numpy in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.22.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Requirement already satisfied: portalocker in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from black->detectron2==0.6) (8.1.7)\n",
      "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from black->detectron2==0.6) (3.10.0)\n",
      "Collecting tomli>=1.1.0 (from black->detectron2==0.6)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from black->detectron2==0.6) (4.7.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (3.4.1)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorboard->detectron2==0.6)\n",
      "  Using cached protobuf-3.19.6-py2.py3-none-any.whl.metadata (828 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (68.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.6.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (2.2.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (0.35.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.26.18)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.2.2)\n",
      "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Using cached iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached pycocotools-2.0.8-cp310-cp310-macosx_10_9_universal2.whl (162 kB)\n",
      "Using cached black-24.10.0-cp310-cp310-macosx_10_9_x86_64.whl (1.6 MB)\n",
      "Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Using cached protobuf-3.19.6-py2.py3-none-any.whl (162 kB)\n",
      "Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: antlr4-python3-runtime, tomli, protobuf, pathspec, omegaconf, mypy-extensions, iopath, cloudpickle, hydra-core, black, pycocotools, detectron2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: iopath\n",
      "    Found existing installation: iopath 0.1.10\n",
      "    Uninstalling iopath-0.1.10:\n",
      "      Successfully uninstalled iopath-0.1.10\n",
      "  Running setup.py develop for detectron2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow 2.10.0 requires tensorflow-io-gcs-filesystem>=0.23.1, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 black-24.10.0 cloudpickle-3.1.0 detectron2-0.6 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 protobuf-3.19.6 pycocotools-2.0.8 tomli-2.2.1\n",
      "Requirement already satisfied: torchvision in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: requests in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torchvision) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: filelock in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->torchvision) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from jinja2->torch==2.1.0->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/detectron2.git\n",
    "!python3 -m pip install -e detectron2\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9028b206-2c46-4959-b7cc-dc271d77a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31aaef0b-e8bb-4a19-b19c-406a163bd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a58825c2-c0dd-4b1a-b486-232b735c37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARS\n",
    "# Note that the LayoutLMv2 checkpoint that we use in this guide has been trained with max_position_embeddings = 512\n",
    "model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057dc2b-9d23-4f16-aaa6-de50faf02561",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e95c981-0d64-484a-b9a2-84e718b88edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6088f2a2bc5642f8ac459653f2c0bc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1d174e15cc42c6a96bc4ae1ab0cb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1119840530c41e1ac2973b769036ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/123M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb5554fd44f4d7e84e4c80c1b830e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/25.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09eadc975a6f4bc7abdddeed8ac58d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c887faab5c274f09b286297cbdc560c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37493dd7d70d45c8b0a292efed5707f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a subset of the DocVQA data set\n",
    "# Full data set can be found at https://rrc.cvc.uab.es/?ch=17\n",
    "dataset = load_dataset(\"nielsr/docvqa_1200_examples\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07dddef-5efa-43b5-b514-f27779620e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is split into train and test sets already\n",
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d8551-f0e3-4357-a656-3824974bfd76",
   "metadata": {},
   "source": [
    "Here’s what the individual fields within the data set represent:\n",
    "\n",
    "- id: the example’s id\n",
    "- image: a PIL.Image.Image object containing the document image\n",
    "- query: the question string - natural language asked question, in several languages\n",
    "- answers: a list of correct answers provided by human annotators\n",
    "- words and bounding_boxes: the results of OCR, which we will not use here\n",
    "- answer: an answer matched by a different model which we will not use here\n",
    "\n",
    "Let’s leave only English questions, and drop the answer feature which appears to contain predictions by another model. We’ll also take the first of the answers from the set provided by the annotators. Alternatively, you can randomly sample it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b328e2-3708-4d58-ae04-00df7918430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = dataset.map(lambda example: {\"question\": example[\"query\"][\"en\"]}, remove_columns=[\"query\"])\n",
    "updated_dataset = updated_dataset.map(\n",
    "    lambda example: {\"answer\": example[\"answers\"][0]}, remove_columns=[\"answer\", \"answers\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ae85b-ade6-4174-87c4-80239f0dc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since max embeddings = 512, we’ll remove the few examples where the embedding is likely to end up longer than 512\n",
    "# Could have truncated the examples the answer might be at the end of a large document and also end up truncated\n",
    "# Alternatively, implement a sliding window strategy: https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
    "updated_dataset = updated_dataset.filter(lambda x: len(x[\"words\"]) + len(x[\"question\"].split()) < 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ac2bc-9c47-4ece-b3e3-391653fb9c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
