{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19e8ca4-b3aa-4d2a-8bcb-32537c2c529f",
   "metadata": {},
   "source": [
    "Image captioning is the task of predicting a caption for a given image. Common real world applications of it include aiding visually impaired people that can help them navigate through different situations. Therefore, image captioning helps to improve content accessibility for people by describing images to them.\n",
    "\n",
    "This guide shows how to:\n",
    "1. Fine-tune an image captioning model.\n",
    "2. Use the fine-tuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92750bfc-7835-413b-8e12-c68394189e04",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11d63d-c206-4584-8209-896d18c4d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate -q\n",
    "pip install jiwer -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9642c1-354e-4ed4-8cab-dd77a01b21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cc91b-3bf6-461d-b2cc-a5fbbe270159",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33769a3-f5cf-4b56-8c1e-fdb4252dc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pok√©mon BLIP captions dataset\n",
    "# Consists of {image-caption} pairs\n",
    "ds = load_dataset(\"lambdalabs/pokemon-blip-captions\")\n",
    "\n",
    "# Inspect data set - note the two features (image and text)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504a470-a7af-48a8-a7ff-661f38433fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c658d-5057-4329-8fc8-5392834650a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise examples from the training set\n",
    "def plot_images(images, captions):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, 12))\n",
    "        plt.title(caption)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "sample_images_to_visualize = [np.array(train_ds[i][\"image\"]) for i in range(5)]\n",
    "sample_captions = [train_ds[i][\"text\"] for i in range(5)]\n",
    "plot_images(sample_images_to_visualize, sample_captions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
