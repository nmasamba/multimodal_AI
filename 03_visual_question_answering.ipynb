{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e70aaa-ea36-4d3d-af61-3f8d93cf9d0c",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. The input to models supporting this task is typically a combination of an image and a question, and the output is an answer expressed in natural language.\n",
    "\n",
    "Some noteworthy use case examples for VQA include:\n",
    "- Accessibility applications for visually impaired individuals.\n",
    "- Education: posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.\n",
    "- Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products.\n",
    "- Image retrieval: VQA models can be used to retrieve images with specific characteristics. For example, the user can ask “Is there a dog?” to find all images with dogs from a set of images.\n",
    "\n",
    "In this guide:\n",
    "1. Fine-tune a classification VQA model, specifically ViLT, on the Graphcore/vqa dataset.\n",
    "2. Use your fine-tuned ViLT for inference.\n",
    "3. Run zero-shot VQA inference with a generative model, like BLIP-2.\n",
    "\n",
    "A note on ViLT versus some recent VQA models: ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier head is placed on top (a linear layer on top of the final hidden state of the [CLS] token) and randomly initialized. Visual Question Answering is thus treated as a classification problem. More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we illustrate how to use them for zero-shot VQA inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c5309-8831-406d-bcf1-1f6e5ddb3b86",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee66dfd-11c7-4286-8c22-ff22e4849f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21265c-f76e-429f-b151-bedbca9a1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from transformers import ViltProcessor, DefaultDataCollator, ViltForQuestionAnswering, TrainingArguments, Trainer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11caf23-d81a-41e4-ac1f-6e4449673c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "MODEL_CHECKPOINT = \"dandelin/vilt-b32-mlm\"\n",
    "\n",
    "# Automatically detect the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n",
    "device, _, _ = get_backend() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcee77f-d277-42c8-b822-4950fb9556bb",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00c6ed-62b9-4f8a-b947-fa4105e013e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a very small sample of the annotated visual question answering Graphcore/vqa dataset\n",
    "dataset = load_dataset(\"Graphcore/vqa\", split=\"validation[:200]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1abf8-081a-4827-b057-ccd4fb9a8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect an example\n",
    "# The features relevant to the task include:\n",
    "# question: the question to be answered from the image\n",
    "# image_id: the path to the image the question refers to\n",
    "# label: the annotations (contains several answers to the same question because answers can be subjective)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9faf3-d0f2-4d6a-9d81-c54f6c14e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the image corresponding to the example above. What label would you have given for the question?\n",
    "image = Image.open(dataset[0]['image_id'])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159d0a0-abbc-455d-9677-4e6a6e0741dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rest of the features as they won't be necessary for this task\n",
    "dataset = dataset.remove_columns(['question_type', 'question_id', 'answer_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab70257-999c-4166-bf52-bc9d5b200379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to Q&As' ambiguity, datasets like this are treated as a multi-label classification problem\n",
    "# Moreover, rather than just creating a one-hot encoded vector, one creates a soft encoding\n",
    "# Soft encoding based on the number of times a certain answer appeared in the annotations\n",
    "labels = [item['ids'] for item in dataset['label']]\n",
    "flattened_labels = list(itertools.chain(*labels))\n",
    "unique_labels = list(set(flattened_labels))\n",
    "\n",
    "# To later instantiate the model with an appropriate classification head, create two dictionaries\n",
    "# One dictionary maps the label name to an integer, and the other reverses this mapping\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b0b9a-ecd1-4684-b22e-4ed9a56c99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the mappings, we can replace the string answers with their ids\n",
    "def replace_ids(inputs):\n",
    "    inputs[\"label\"][\"ids\"] = [label2id[x] for x in inputs[\"label\"][\"ids\"]]\n",
    "    return inputs\n",
    "\n",
    "dataset = dataset.map(replace_ids)\n",
    "flat_dataset = dataset.flatten()\n",
    "flat_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97d0ea-4b2a-44a6-9ee3-0021ccb57c48",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0f290-f19c-4381-b3b7-43a209a18f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a ViLT processor to prepare the image and text data\n",
    "# ViltProcessor wraps a BERT tokenizer and ViLT image processor into a convenient single processor\n",
    "processor = ViltProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1f7ff-9429-41cc-95f1-87a2cb5f74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare the target labels such that each element corresponds to a possible answer (label)\n",
    "# For correct answers, the element holds their respective score (weight)\n",
    "# For incprrect answers, the element weights are set to zero\n",
    "def preprocess_data(examples):\n",
    "    image_paths = examples['image_id']\n",
    "    images = [Image.open(image_path) for image_path in image_paths]\n",
    "    texts = examples['question']\n",
    "\n",
    "    encoding = processor(images, texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    for k, v in encoding.items():\n",
    "          encoding[k] = v.squeeze()\n",
    "\n",
    "    targets = []\n",
    "\n",
    "    for labels, scores in zip(examples['label.ids'], examples['label.weights']):\n",
    "        target = torch.zeros(len(id2label))\n",
    "\n",
    "        for label, score in zip(labels, scores):\n",
    "            target[label] = score\n",
    "\n",
    "        targets.append(target)\n",
    "\n",
    "    encoding[\"labels\"] = targets\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad76696-88a1-49b1-9576-22e93bb4ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to dataset and remove unwanted columns\n",
    "cols_to_remove = ['question','question_type',  'question_id', 'image_id', 'answer_type', 'label.ids', 'label.weights']\n",
    "processed_dataset = flat_dataset.map(preprocess_data, \n",
    "                                     batched=True, \n",
    "                                     remove_columns=cols_to_remove)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3716f-6833-4e99-988a-195efe098861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of examples\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39080d2-334d-4be8-b31d-ab6516474c3a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1124781-7835-4713-aa46-b14facb3d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ViLT with ViltForQuestionAnswering\n",
    "# Specify the number of labels along with the label mappings\n",
    "model = ViltForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT, \n",
    "                                                 num_labels=len(id2label), \n",
    "                                                 id2label=id2label, \n",
    "                                                 label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee27f47-6672-4a03-a920-1c81a7e2e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training hyperparameters in TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"vqa_vilt_finetuned\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=20,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8109635-2183-442c-a265-29f40f4de237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the training arguments to Trainer along with the model, dataset, processor, and data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=processed_dataset,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8531a2f-b39e-483c-8d11-2430967e8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call train() to finetune your model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7df32-a983-431b-85d3-bd1fcc7d0013",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3bebb6-a20d-4782-93ad-1a2c87c7f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using a pipeline\n",
    "pipe = pipeline(\"visual-question-answering\", model=\"vqa_vilt_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95573ebc-205a-450f-9f58-4f5bc7b3e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check inference on first example\n",
    "# Note that the model was trained on only 200 examples so performance won't be optimal\n",
    "example = dataset[0]\n",
    "image = Image.open(example['image_id'])\n",
    "question = example['question']\n",
    "print(question)\n",
    "pipe(image, question, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca17a3a-45dc-49c3-9c85-7c1c6aa4d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using manual loop\n",
    "processor = ViltProcessor.from_pretrained(\"vqa_vilt_finetuned\")\n",
    "\n",
    "image = Image.open(example['image_id'])\n",
    "question = example['question']\n",
    "\n",
    "# prepare inputs\n",
    "inputs = processor(image, question, return_tensors='pt')\n",
    "\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"vqa_vilt_finetuned\")\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "idx = logits.argmax(-1).item()\n",
    "print(\"Predicted answer:\", model.config.id2label[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ac604-5243-4c1e-959a-871327de164b",
   "metadata": {},
   "source": [
    "# Zero-shot VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef33f57-37eb-4f89-997e-1aea5ce9a5bf",
   "metadata": {},
   "source": [
    "The above model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach VQA as a generative task. Let’s take BLIP-2 as an example. It introduced a new visual-language pre-training paradigm in which any combination of pre-trained vision encoder and LLM can be used. This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. Let’s illustrate how you can use the BLIP-2 model for VQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265c39b-1f49-4836-b54c-806d47205222",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304e615-09d9-47f6-8fb0-ba5bb1870428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
