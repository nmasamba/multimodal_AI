{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e70aaa-ea36-4d3d-af61-3f8d93cf9d0c",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. The input to models supporting this task is typically a combination of an image and a question, and the output is an answer expressed in natural language.\n",
    "\n",
    "Some noteworthy use case examples for VQA include:\n",
    "- Accessibility applications for visually impaired individuals.\n",
    "- Education: posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.\n",
    "- Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products.\n",
    "- Image retrieval: VQA models can be used to retrieve images with specific characteristics. For example, the user can ask “Is there a dog?” to find all images with dogs from a set of images.\n",
    "\n",
    "In this guide:\n",
    "1. Fine-tune a classification VQA model, specifically ViLT, on the Graphcore/vqa dataset.\n",
    "2. Use your fine-tuned ViLT for inference.\n",
    "3. Run zero-shot VQA inference with a generative model, like BLIP-2.\n",
    "\n",
    "A note on ViLT versus some recent VQA models: ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier head is placed on top (a linear layer on top of the final hidden state of the [CLS] token) and randomly initialized. Visual Question Answering is thus treated as a classification problem. More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we illustrate how to use them for zero-shot VQA inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c5309-8831-406d-bcf1-1f6e5ddb3b86",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee66dfd-11c7-4286-8c22-ff22e4849f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11caf23-d81a-41e4-ac1f-6e4449673c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "MODEL_CHECKPOINT = \"dandelin/vilt-b32-mlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6275ca-b3ae-47dd-b292-68188b7c1567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00c6ed-62b9-4f8a-b947-fa4105e013e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1abf8-081a-4827-b057-ccd4fb9a8890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
