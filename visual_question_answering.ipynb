{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e70aaa-ea36-4d3d-af61-3f8d93cf9d0c",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. The input to models supporting this task is typically a combination of an image and a question, and the output is an answer expressed in natural language.\n",
    "\n",
    "Some noteworthy use case examples for VQA include:\n",
    "- Accessibility applications for visually impaired individuals.\n",
    "- Education: posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.\n",
    "- Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products.\n",
    "- Image retrieval: VQA models can be used to retrieve images with specific characteristics. For example, the user can ask “Is there a dog?” to find all images with dogs from a set of images.\n",
    "\n",
    "In this guide:\n",
    "1. Fine-tune a classification VQA model, specifically ViLT, on the Graphcore/vqa dataset.\n",
    "2. Use your fine-tuned ViLT for inference.\n",
    "3. Run zero-shot VQA inference with a generative model, like BLIP-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3fa924-48db-4309-9a37-bc31e987e988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
